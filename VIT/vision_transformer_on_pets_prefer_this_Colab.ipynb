{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTDvAeHGoJS3"
   },
   "source": [
    "# Vision Transformers from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHZBgsNCdCRk"
   },
   "source": [
    "- [ViT Blogpost by Francesco Zuppichini](https://towardsdatascience.com/implementing-visualttransformer-in-pytorch-184f9f16f632)\n",
    "- [D2L Tutorial ](https://d2l.ai/chapter_attention-mechanisms-and-transformers/vision-transformer.html)\n",
    "- [Brian Pulfer Medium Blogpost](https://medium.com/mlearning-ai/vision-transformers-from-scratch-pytorch-a-step-by-step-guide-96c3313c2e0c)\n",
    "- [Lucidrains implementation Github ](https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wdEfT7i40Eka"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xSA9DeQ-0GJk",
    "outputId": "963440f1-0340-4a4f-c25d-bc73dcd6f971"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2k5RT2l4XuN"
   },
   "source": [
    "## Image Patching\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "w0a8TAbg3KQd"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import OxfordIIITPet\n",
    "import matplotlib.pyplot as plt\n",
    "from random import random\n",
    "from torchvision.transforms import Resize, ToTensor\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "to_tensor = [Resize((144, 144)), ToTensor()]\n",
    "\n",
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image = t(image)\n",
    "        return image, target\n",
    "\n",
    "def show_images(images, num_samples=40, cols=8):\n",
    "    \"\"\" Plots some samples from the dataset \"\"\"\n",
    "    plt.figure(figsize=(15,15))\n",
    "    idx = int(len(dataset) / num_samples)\n",
    "    print(images)\n",
    "    for i, img in enumerate(images):\n",
    "        if i % idx == 0:\n",
    "            plt.subplot(int(num_samples/cols) + 1, cols, int(i/idx) + 1)\n",
    "            plt.imshow(to_pil_image(img[0]))\n",
    "\n",
    "# 200 images for each pet\n",
    "dataset = OxfordIIITPet(root=\".\", download=True, transforms=Compose(to_tensor))\n",
    "show_images(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2WPMD1CTx5lL"
   },
   "source": [
    "### Patch Images\n",
    "\n",
    "- The following is mainly from the above implementations (not my code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Ap3RzZa0yZXt"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from einops.layers.torch import Rearrange\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels = 3, patch_size = 8, emb_size = 128):\n",
    "        self.patch_size = patch_size\n",
    "        super().__init__()\n",
    "        self.projection = nn.Sequential(\n",
    "            # break-down the image in s1 x s2 patches and flat them\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_size, p2=patch_size),\n",
    "            nn.Linear(patch_size * patch_size * in_channels, emb_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.projection(x)\n",
    "        return x\n",
    "\n",
    "# Run a quick test\n",
    "sample_datapoint = torch.unsqueeze(dataset[0][0], 0)\n",
    "print(\"Initial shape: \", sample_datapoint.shape)\n",
    "embedding = PatchEmbedding()(sample_datapoint)\n",
    "print(\"Patches shape: \", embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LyXkSfe84dEj"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ohxDCNNsim0"
   },
   "source": [
    "Let's first implement all of the transformer building blocks. These blocks are inspired by the implementations linked above. I've left out some dropouts and normalizations at some places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "lbKH_cEt_AGF"
   },
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, n_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.att = torch.nn.MultiheadAttention(embed_dim=dim,\n",
    "                                               num_heads=n_heads,\n",
    "                                               dropout=dropout)\n",
    "        self.q = torch.nn.Linear(dim, dim)\n",
    "        self.k = torch.nn.Linear(dim, dim)\n",
    "        self.v = torch.nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = self.q(x)\n",
    "        k = self.k(x)\n",
    "        v = self.v(x)\n",
    "        attn_output, attn_output_weights = self.att(x, x, x)\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "i_7Q0Quj_Ch8"
   },
   "outputs": [],
   "source": [
    "Attention(dim=128, n_heads=4, dropout=0.)(torch.ones((1, 5, 128))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "LZIq8qATHX7p"
   },
   "outputs": [],
   "source": [
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "TpYpgqCqHcrs"
   },
   "outputs": [],
   "source": [
    "norm = PreNorm(128, Attention(dim=128, n_heads=4, dropout=0.))\n",
    "norm(torch.ones((1, 5, 128))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "XGsxbcqxIGJg"
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Sequential):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "ff = FeedForward(dim=128, hidden_dim=256)\n",
    "ff(torch.ones((1, 5, 128))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "WuWewoFfI7Vs"
   },
   "outputs": [],
   "source": [
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "hiv5BDUAI73N"
   },
   "outputs": [],
   "source": [
    "residual_att = ResidualAdd(Attention(dim=128, n_heads=4, dropout=0.))\n",
    "residual_att(torch.ones((1, 5, 128))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qewws3-Nk7zB"
   },
   "source": [
    "- Not all parameters are like in the original implementation\n",
    "- Some Dropouts & Norms are missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "KVVWmPso4eGf"
   },
   "outputs": [],
   "source": [
    "from einops import repeat\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, ch=3, img_size=144, patch_size=4, emb_dim=32,\n",
    "                n_layers=6, out_dim=37, dropout=0.1, heads=2):\n",
    "        super(ViT, self).__init__()\n",
    "\n",
    "        # Attributes\n",
    "        self.channels = ch\n",
    "        self.height = img_size\n",
    "        self.width = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Patching\n",
    "        self.patch_embedding = PatchEmbedding(in_channels=ch,\n",
    "                                              patch_size=patch_size,\n",
    "                                              emb_size=emb_dim)\n",
    "        # Learnable params\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "        self.pos_embedding = nn.Parameter(\n",
    "            torch.randn(1, num_patches + 1, emb_dim))\n",
    "        self.cls_token = nn.Parameter(torch.rand(1, 1, emb_dim))\n",
    "\n",
    "        # Transformer Encoder\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(n_layers):\n",
    "            transformer_block = nn.Sequential(\n",
    "                ResidualAdd(PreNorm(emb_dim, Attention(emb_dim, n_heads = heads, dropout = dropout))),\n",
    "                ResidualAdd(PreNorm(emb_dim, FeedForward(emb_dim, emb_dim, dropout = dropout))))\n",
    "            self.layers.append(transformer_block)\n",
    "\n",
    "        # Classification head\n",
    "        self.head = nn.Sequential(nn.LayerNorm(emb_dim), nn.Linear(emb_dim, out_dim))\n",
    "\n",
    "\n",
    "    def forward(self, img):\n",
    "        # Get patch embedding vectors\n",
    "        x = self.patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        # Add cls token to inputs\n",
    "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "\n",
    "        # Transformer layers\n",
    "        for i in range(self.n_layers):\n",
    "            x = self.layers[i](x)\n",
    "\n",
    "        # Output based on classification token\n",
    "        return self.head(x[:, 0, :])\n",
    "\n",
    "\n",
    "model = ViT()\n",
    "print(model)\n",
    "model(torch.ones((1, 3, 144, 144)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6j2tczm4fGs"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "FsdvFNcr4iFj"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "train_split = int(0.8 * len(dataset))\n",
    "train, test = random_split(dataset, [train_split, len(dataset) - train_split])\n",
    "\n",
    "train_dataloader = DataLoader(train, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N-Q_3rEJSOfA",
    "outputId": "6c88d66f-fb6a-4fa3-c78c-8610add43bad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 0 train loss:  3.6508754025334897\n",
      ">>> Epoch 0 test loss:  3.6382168272267217\n",
      ">>> Epoch 5 train loss:  3.626315210176551\n",
      ">>> Epoch 5 test loss:  3.6225407641866934\n",
      ">>> Epoch 10 train loss:  3.620842220990554\n",
      ">>> Epoch 10 test loss:  3.625303620877473\n",
      ">>> Epoch 15 train loss:  3.621863606183425\n",
      ">>> Epoch 15 test loss:  3.62233012655507\n",
      ">>> Epoch 20 train loss:  3.6195932367573613\n",
      ">>> Epoch 20 test loss:  3.6210498187852944\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "device = \"cuda\"\n",
    "model = ViT().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1000):\n",
    "    epoch_losses = []\n",
    "    model.train()\n",
    "    for step, (inputs, labels) in enumerate(train_dataloader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\">>> Epoch {epoch} train loss: \", np.mean(epoch_losses))\n",
    "        epoch_losses = []\n",
    "        # Something was strange when using this?\n",
    "        # model.eval()\n",
    "        for step, (inputs, labels) in enumerate(test_dataloader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            epoch_losses.append(loss.item())\n",
    "        print(f\">>> Epoch {epoch} test loss: \", np.mean(epoch_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V7LkzG6fOt_k",
    "outputId": "10e5aacc-094a-4554-b4ec-4696fefaeae6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted classes tensor([26, 33, 15, 33, 19, 15, 19, 19], device='cuda:0')\n",
      "Actual classes tensor([32, 22, 23, 23, 20, 13, 31, 12], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "inputs, labels = next(iter(test_dataloader))\n",
    "inputs, labels = inputs.to(device), labels.to(device)\n",
    "outputs = model(inputs)\n",
    "\n",
    "print(\"Predicted classes\", outputs.argmax(-1))\n",
    "print(\"Actual classes\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sM9VzohRCrC_"
   },
   "source": [
    "This needs to train much longer :)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
