{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "token = os.getenv('PERPLEXITYAI_API_KEY')\n",
    "GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\")\n",
    "TAVILY_API_KEY = os.environ.get(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast language models, also known as rapid language models or super-speed language models, have gained significant attention in recent years due to their ability to process natural language input quickly and efficiently. The importance of fast language models lies in their potential to revolutionize various applications that rely on natural language processing (NLP), including but not limited to:\n",
      "\n",
      "1. **Conversational AI**: Fast language models enable developers to build conversational AI systems that can respond to user queries in real-time, achieving faster response times and a more natural conversation flow.\n",
      "2. **Real-time language translation**: Fast language models can translate text or speech in a fraction of a second, facilitating real-time communication across language barriers.\n",
      "3. **Sentiment analysis**: Fast language models can analyze vast amounts of text data quickly, providing insights into customer feedback, sentiment, and trends, enabling businesses to make data-driven decisions.\n",
      "4. **Chatbots and virtual assistants**: Fast language models can power chatbots and virtual assistants to provide faster and more accurate responses to user queries, improving the overall user experience.\n",
      "5. **Spam detection and cybersecurity**: Fast language models can detect spam emails, messages, and malicious code more efficiently, helping to improve cybersecurity and reduce the risk of attacks.\n",
      "6. **Content generation and summarization**: Fast language models can generate content (e.g., articles, social media posts) and summarize long documents quickly, streamlining content creation and analysis processes.\n",
      "7. **Emotional intelligence and psychology**: Fast language models can analyze emotional cues, such as sentiment, tone, and language patterns, to better understand human emotions and behavior.\n",
      "8. **Healthcare and medical research**: Fast language models can process patient records, medical reports, and research papers quickly, facilitating faster diagnosis, treatment, and research outcomes.\n",
      "9. **Customer service**: Fast language models can provide automated customer support, responding to customer inquiries and issues in a timely and efficient manner.\n",
      "10. **Education and language learning**: Fast language models can help teach languages more effectively by providing instant grammar and syntax feedback, as well as personalized language lessons.\n",
      "\n",
      "The importance of fast language models can be attributed to their ability to:\n",
      "\n",
      "1. Process large volumes of text data quickly\n",
      "2. Analyze complex language patterns and relationships\n",
      "3. Recognize and respond to user input efficiently\n",
      "4. Provide accurate and relevant results in real-time\n",
      "5. Enable real-time communication and collaboration\n",
      "6. Improve the overall user experience through faster responses and more accurate information\n",
      "\n",
      "The rapid development of fast language models is fueled by advances in deep learning, parallel processing, and cloud computing. As these technologies continue to evolve, we can expect to see even more innovative applications of fast language models in various industries and domains.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-8b-8192\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting altair==5.3.0 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 7))\n",
      "  Downloading altair-5.3.0-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: annotated-types==0.6.0 in /home/elnur/anaconda3/lib/python3.11/site-packages (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 9)) (0.6.0)\n",
      "Collecting anyio==4.3.0 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 11))\n",
      "  Downloading anyio-4.3.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting attrs==23.2.0 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 15))\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting blinker==1.7.0 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 19))\n",
      "  Using cached blinker-1.7.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting cachetools==5.3.3 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 21))\n",
      "  Downloading cachetools-5.3.3-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting certifi==2024.2.2 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 23))\n",
      "  Using cached certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting charset-normalizer==3.3.2 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 28))\n",
      "  Using cached charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
      "Requirement already satisfied: click==8.1.7 in /home/elnur/anaconda3/lib/python3.11/site-packages (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 30)) (8.1.7)\n",
      "Collecting distro==1.9.0 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 34))\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting exceptiongroup==1.2.1 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 36))\n",
      "  Downloading exceptiongroup-1.2.1-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting gitdb==4.0.11 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 38))\n",
      "  Using cached gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting gitpython==3.1.43 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 40))\n",
      "  Using cached GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting groq==0.5.0 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 44))\n",
      "  Downloading groq-0.5.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: h11==0.14.0 in /home/elnur/anaconda3/lib/python3.11/site-packages (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 46)) (0.14.0)\n",
      "Collecting httpcore==1.0.5 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 48))\n",
      "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting httpx==0.27.0 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 50))\n",
      "  Using cached httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting idna==3.7 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 54))\n",
      "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: jinja2==3.1.3 in /home/elnur/anaconda3/lib/python3.11/site-packages (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 59)) (3.1.3)\n",
      "Collecting jsonschema==4.21.1 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 63))\n",
      "  Using cached jsonschema-4.21.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting jsonschema-specifications==2023.12.1 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 65))\n",
      "  Using cached jsonschema_specifications-2023.12.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting markdown-it-py==3.0.0 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 67))\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting markupsafe==2.1.5 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 69))\n",
      "  Using cached MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting mdurl==0.1.2 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 71))\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: numpy==1.26.4 in /home/elnur/anaconda3/lib/python3.11/site-packages (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 73)) (1.26.4)\n",
      "Collecting packaging==24.0 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 80))\n",
      "  Downloading packaging-24.0-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting pandas==2.2.2 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 84))\n",
      "  Downloading pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting phidata==2.4.20 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 88))\n",
      "  Downloading phidata-2.4.20-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting pillow==10.3.0 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 90))\n",
      "  Downloading pillow-10.3.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting protobuf==4.25.3 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 92))\n",
      "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Collecting pyarrow==16.0.0 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 94))\n",
      "  Downloading pyarrow-16.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting pydantic==2.7.0 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 96))\n",
      "  Downloading pydantic-2.7.0-py3-none-any.whl.metadata (103 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.4/103.4 kB\u001b[0m \u001b[31m734.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pydantic-core==2.18.1 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 101))\n",
      "  Downloading pydantic_core-2.18.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting pydantic-settings==2.2.1 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 103))\n",
      "  Downloading pydantic_settings-2.2.1-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting pydeck==0.8.1b0 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 105))\n",
      "  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting pygments==2.17.2 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 107))\n",
      "  Using cached pygments-2.17.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting python-dateutil==2.9.0.post0 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 109))\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting python-dotenv==1.0.1 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 111))\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting pytz==2024.1 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 115))\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: pyyaml==6.0.1 in /home/elnur/anaconda3/lib/python3.11/site-packages (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 117)) (6.0.1)\n",
      "Collecting referencing==0.34.0 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 119))\n",
      "  Downloading referencing-0.34.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting regex==2024.4.16 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 123))\n",
      "  Downloading regex-2024.4.16-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests==2.31.0 in /home/elnur/anaconda3/lib/python3.11/site-packages (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 125)) (2.31.0)\n",
      "Collecting rich==13.7.1 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 130))\n",
      "  Using cached rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting rpds-py==0.18.0 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 135))\n",
      "  Using cached rpds_py-0.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting shellingham==1.5.4 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 139))\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: six==1.16.0 in /home/elnur/anaconda3/lib/python3.11/site-packages (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 141)) (1.16.0)\n",
      "Collecting smmap==5.0.1 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 143))\n",
      "  Using cached smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting sniffio==1.3.1 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 145))\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting streamlit==1.33.0 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 150))\n",
      "  Downloading streamlit-1.33.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting tavily-python==0.3.3 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 152))\n",
      "  Downloading tavily_python-0.3.3-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting tenacity==8.2.3 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 154))\n",
      "  Using cached tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting tiktoken==0.6.0 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 156))\n",
      "  Downloading tiktoken-0.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: toml==0.10.2 in /home/elnur/anaconda3/lib/python3.11/site-packages (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 158)) (0.10.2)\n",
      "Collecting tomli==2.0.1 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 160))\n",
      "  Using cached tomli-2.0.1-py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting toolz==0.12.1 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 162))\n",
      "  Downloading toolz-0.12.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting tornado==6.4 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 164))\n",
      "  Using cached tornado-6.4-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting typer==0.12.3 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 166))\n",
      "  Downloading typer-0.12.3-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting typing-extensions==4.11.0 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 168))\n",
      "  Downloading typing_extensions-4.11.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting tzdata==2024.1 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 178))\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting urllib3==1.26.18 (from -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 180))\n",
      "  Downloading urllib3-1.26.18-py2.py3-none-any.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: watchdog>=2.1.5 in /home/elnur/anaconda3/lib/python3.11/site-packages (from streamlit==1.33.0->-r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt (line 150)) (2.1.6)\n",
      "Downloading altair-5.3.0-py3-none-any.whl (857 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m857.8/857.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading anyio-4.3.0-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.6/85.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Using cached blinker-1.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (140 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading exceptiongroup-1.2.1-py3-none-any.whl (16 kB)\n",
      "Using cached gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Using cached GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "Downloading groq-0.5.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "Downloading idna-3.7-py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m649.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m--:--\u001b[0m\n",
      "\u001b[?25hUsing cached jsonschema-4.21.1-py3-none-any.whl (85 kB)\n",
      "Using cached jsonschema_specifications-2023.12.1-py3-none-any.whl (18 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading packaging-24.0-py3-none-any.whl (53 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.5/53.5 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading phidata-2.4.20-py3-none-any.whl (580 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m580.4/580.4 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pillow-10.3.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-16.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (40.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.7.0-py3-none-any.whl (407 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m407.9/407.9 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.18.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_settings-2.2.1-py3-none-any.whl (13 kB)\n",
      "Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pygments-2.17.2-py3-none-any.whl (1.2 MB)\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Downloading referencing-0.34.0-py3-none-any.whl (26 kB)\n",
      "Downloading regex-2024.4.16-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (785 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m785.1/785.1 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "Using cached rpds_py-0.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading streamlit-1.33.0-py2.py3-none-any.whl (8.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tavily_python-0.3.3-py3-none-any.whl (5.4 kB)\n",
      "Using cached tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Downloading tiktoken-0.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tomli-2.0.1-py3-none-any.whl (12 kB)\n",
      "Downloading toolz-0.12.1-py3-none-any.whl (56 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached tornado-6.4-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, urllib3, tzdata, typing-extensions, tornado, toolz, tomli, tenacity, sniffio, smmap, shellingham, rpds-py, regex, python-dotenv, python-dateutil, pygments, pyarrow, protobuf, pillow, packaging, mdurl, markupsafe, idna, exceptiongroup, distro, charset-normalizer, certifi, cachetools, blinker, attrs, referencing, pydantic-core, pandas, markdown-it-py, httpcore, gitdb, anyio, tiktoken, rich, pydeck, pydantic, jsonschema-specifications, httpx, gitpython, typer, tavily-python, pydantic-settings, jsonschema, groq, phidata, altair, streamlit\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2023.3.post1\n",
      "    Uninstalling pytz-2023.3.post1:\n",
      "      Successfully uninstalled pytz-2023.3.post1\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.7\n",
      "    Uninstalling urllib3-2.0.7:\n",
      "      Successfully uninstalled urllib3-2.0.7\n",
      "  Attempting uninstall: tzdata\n",
      "    Found existing installation: tzdata 2023.3\n",
      "    Uninstalling tzdata-2023.3:\n",
      "      Successfully uninstalled tzdata-2023.3\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.12.2\n",
      "    Uninstalling typing_extensions-4.12.2:\n",
      "      Successfully uninstalled typing_extensions-4.12.2\n",
      "  Attempting uninstall: tornado\n",
      "    Found existing installation: tornado 6.3.3\n",
      "    Uninstalling tornado-6.3.3:\n",
      "      Successfully uninstalled tornado-6.3.3\n",
      "  Attempting uninstall: toolz\n",
      "    Found existing installation: toolz 0.12.0\n",
      "    Uninstalling toolz-0.12.0:\n",
      "      Successfully uninstalled toolz-0.12.0\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 8.2.2\n",
      "    Uninstalling tenacity-8.2.2:\n",
      "      Successfully uninstalled tenacity-8.2.2\n",
      "  Attempting uninstall: sniffio\n",
      "    Found existing installation: sniffio 1.3.0\n",
      "    Uninstalling sniffio-1.3.0:\n",
      "      Successfully uninstalled sniffio-1.3.0\n",
      "  Attempting uninstall: smmap\n",
      "    Found existing installation: smmap 4.0.0\n",
      "    Uninstalling smmap-4.0.0:\n",
      "      Successfully uninstalled smmap-4.0.0\n",
      "  Attempting uninstall: rpds-py\n",
      "    Found existing installation: rpds-py 0.10.6\n",
      "    Uninstalling rpds-py-0.10.6:\n",
      "      Successfully uninstalled rpds-py-0.10.6\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2023.10.3\n",
      "    Uninstalling regex-2023.10.3:\n",
      "      Successfully uninstalled regex-2023.10.3\n",
      "  Attempting uninstall: python-dotenv\n",
      "    Found existing installation: python-dotenv 0.21.0\n",
      "    Uninstalling python-dotenv-0.21.0:\n",
      "      Successfully uninstalled python-dotenv-0.21.0\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.8.2\n",
      "    Uninstalling python-dateutil-2.8.2:\n",
      "      Successfully uninstalled python-dateutil-2.8.2\n",
      "  Attempting uninstall: pygments\n",
      "    Found existing installation: Pygments 2.15.1\n",
      "    Uninstalling Pygments-2.15.1:\n",
      "      Successfully uninstalled Pygments-2.15.1\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 14.0.2\n",
      "    Uninstalling pyarrow-14.0.2:\n",
      "      Successfully uninstalled pyarrow-14.0.2\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: pillow 10.2.0\n",
      "    Uninstalling pillow-10.2.0:\n",
      "      Successfully uninstalled pillow-10.2.0\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.1\n",
      "    Uninstalling packaging-23.1:\n",
      "      Successfully uninstalled packaging-23.1\n",
      "  Attempting uninstall: mdurl\n",
      "    Found existing installation: mdurl 0.1.0\n",
      "    Uninstalling mdurl-0.1.0:\n",
      "      Successfully uninstalled mdurl-0.1.0\n",
      "  Attempting uninstall: markupsafe\n",
      "    Found existing installation: MarkupSafe 2.1.3\n",
      "    Uninstalling MarkupSafe-2.1.3:\n",
      "      Successfully uninstalled MarkupSafe-2.1.3\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.4\n",
      "    Uninstalling idna-3.4:\n",
      "      Successfully uninstalled idna-3.4\n",
      "  Attempting uninstall: distro\n",
      "    Found existing installation: distro 1.8.0\n",
      "    Uninstalling distro-1.8.0:\n",
      "      Successfully uninstalled distro-1.8.0\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 2.0.4\n",
      "    Uninstalling charset-normalizer-2.0.4:\n",
      "      Successfully uninstalled charset-normalizer-2.0.4\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2024.8.30\n",
      "    Uninstalling certifi-2024.8.30:\n",
      "      Successfully uninstalled certifi-2024.8.30\n",
      "  Attempting uninstall: cachetools\n",
      "    Found existing installation: cachetools 4.2.2\n",
      "    Uninstalling cachetools-4.2.2:\n",
      "      Successfully uninstalled cachetools-4.2.2\n",
      "  Attempting uninstall: blinker\n",
      "    Found existing installation: blinker 1.6.2\n",
      "    Uninstalling blinker-1.6.2:\n",
      "      Successfully uninstalled blinker-1.6.2\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 23.1.0\n",
      "    Uninstalling attrs-23.1.0:\n",
      "      Successfully uninstalled attrs-23.1.0\n",
      "  Attempting uninstall: referencing\n",
      "    Found existing installation: referencing 0.30.2\n",
      "    Uninstalling referencing-0.30.2:\n",
      "      Successfully uninstalled referencing-0.30.2\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.20.1\n",
      "    Uninstalling pydantic_core-2.20.1:\n",
      "      Successfully uninstalled pydantic_core-2.20.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.1.4\n",
      "    Uninstalling pandas-2.1.4:\n",
      "      Successfully uninstalled pandas-2.1.4\n",
      "  Attempting uninstall: markdown-it-py\n",
      "    Found existing installation: markdown-it-py 2.2.0\n",
      "    Uninstalling markdown-it-py-2.2.0:\n",
      "      Successfully uninstalled markdown-it-py-2.2.0\n",
      "  Attempting uninstall: httpcore\n",
      "    Found existing installation: httpcore 1.0.2\n",
      "    Uninstalling httpcore-1.0.2:\n",
      "      Successfully uninstalled httpcore-1.0.2\n",
      "  Attempting uninstall: gitdb\n",
      "    Found existing installation: gitdb 4.0.7\n",
      "    Uninstalling gitdb-4.0.7:\n",
      "      Successfully uninstalled gitdb-4.0.7\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 4.2.0\n",
      "    Uninstalling anyio-4.2.0:\n",
      "      Successfully uninstalled anyio-4.2.0\n",
      "  Attempting uninstall: rich\n",
      "    Found existing installation: rich 13.3.5\n",
      "    Uninstalling rich-13.3.5:\n",
      "      Successfully uninstalled rich-13.3.5\n",
      "  Attempting uninstall: pydeck\n",
      "    Found existing installation: pydeck 0.8.0\n",
      "    Uninstalling pydeck-0.8.0:\n",
      "      Successfully uninstalled pydeck-0.8.0\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.8.2\n",
      "    Uninstalling pydantic-2.8.2:\n",
      "      Successfully uninstalled pydantic-2.8.2\n",
      "  Attempting uninstall: jsonschema-specifications\n",
      "    Found existing installation: jsonschema-specifications 2023.7.1\n",
      "    Uninstalling jsonschema-specifications-2023.7.1:\n",
      "      Successfully uninstalled jsonschema-specifications-2023.7.1\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.26.0\n",
      "    Uninstalling httpx-0.26.0:\n",
      "      Successfully uninstalled httpx-0.26.0\n",
      "  Attempting uninstall: gitpython\n",
      "    Found existing installation: GitPython 3.1.37\n",
      "    Uninstalling GitPython-3.1.37:\n",
      "      Successfully uninstalled GitPython-3.1.37\n",
      "  Attempting uninstall: jsonschema\n",
      "    Found existing installation: jsonschema 4.19.2\n",
      "    Uninstalling jsonschema-4.19.2:\n",
      "      Successfully uninstalled jsonschema-4.19.2\n",
      "  Attempting uninstall: groq\n",
      "    Found existing installation: groq 0.11.0\n",
      "    Uninstalling groq-0.11.0:\n",
      "      Successfully uninstalled groq-0.11.0\n",
      "  Attempting uninstall: altair\n",
      "    Found existing installation: altair 5.0.1\n",
      "    Uninstalling altair-5.0.1:\n",
      "      Successfully uninstalled altair-5.0.1\n",
      "  Attempting uninstall: streamlit\n",
      "    Found existing installation: streamlit 1.37.1\n",
      "    Uninstalling streamlit-1.37.1:\n",
      "      Successfully uninstalled streamlit-1.37.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.75 requires requests_mock, which is not installed.\n",
      "conda-repo-cli 1.0.75 requires clyent==1.2.1, but you have clyent 1.2.2 which is incompatible.\n",
      "conda-repo-cli 1.0.75 requires python-dateutil==2.8.2, but you have python-dateutil 2.9.0.post0 which is incompatible.\n",
      "mdit-py-plugins 0.3.0 requires markdown-it-py<3.0.0,>=1.0.0, but you have markdown-it-py 3.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed altair-5.3.0 anyio-4.3.0 attrs-23.2.0 blinker-1.7.0 cachetools-5.3.3 certifi-2024.2.2 charset-normalizer-3.3.2 distro-1.9.0 exceptiongroup-1.2.1 gitdb-4.0.11 gitpython-3.1.43 groq-0.5.0 httpcore-1.0.5 httpx-0.27.0 idna-3.7 jsonschema-4.21.1 jsonschema-specifications-2023.12.1 markdown-it-py-3.0.0 markupsafe-2.1.5 mdurl-0.1.2 packaging-24.0 pandas-2.2.2 phidata-2.4.20 pillow-10.3.0 protobuf-4.25.3 pyarrow-16.0.0 pydantic-2.5.3 pydantic-core-2.18.1 pydantic-settings-2.2.1 pydeck-0.8.1b0 pygments-2.17.2 python-dateutil-2.9.0.post0 python-dotenv-1.0.1 pytz-2024.1 referencing-0.34.0 regex-2024.4.16 rich-13.7.1 rpds-py-0.18.0 shellingham-1.5.4 smmap-5.0.1 sniffio-1.3.1 streamlit-1.33.0 tavily-python-0.3.3 tenacity-8.2.3 tiktoken-0.6.0 tomli-2.0.1 toolz-0.12.1 tornado-6.4 typer-0.12.3 typing-extensions-4.11.0 tzdata-2024.1 urllib3-1.26.18\n"
     ]
    }
   ],
   "source": [
    "!pip install -r /home/elnur/Desktop/DataScience-Bootcamp/langchain_tutorial/phidata/cookbook/llms/groq/research/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
